"""
Air Quality Data Processing Pipeline
===================================

This module provides robust data cleaning and transformation pipeline for
multi-source data fusion combining:
- NASA TEMPO satellite data
- EPA ground station data  
- Weather data

Features:
- Missing value handling and interpolation
- Outlier detection and correction
- Spatial-temporal data fusion
- Feature engineering for ML models
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import KNNImputer
from sklearn.ensemble import IsolationForest
from scipy import interpolate, stats
from datetime import datetime, timedelta
import warnings
from typing import Dict, List, Tuple, Optional, Union
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AirQualityDataProcessor:
    """
    Comprehensive data processing pipeline for air quality forecasting
    """
    
    def __init__(self, sequence_length: int = 24):
        """
        Initialize the data processor
        
        Args:
            sequence_length: Lookback window size for time series (hours)
        """
        self.sequence_length = sequence_length
        self.scaler = StandardScaler()
        self.feature_scaler = MinMaxScaler()
        self.knn_imputer = KNNImputer(n_neighbors=5)
        self.outlier_detector = IsolationForest(contamination=0.1, random_state=42)
        
        # Data quality thresholds
        self.quality_thresholds = {
            'missing_data_max': 0.05,  # 5% maximum missing data
            'outlier_z_score': 3.0,    # Z-score threshold for outliers
            'temporal_gap_max': 3,     # Maximum hours gap for interpolation
            'spatial_distance_max': 50 # Maximum km for spatial interpolation
        }
        
        logger.info(f"Initialized AirQualityDataProcessor with sequence_length={sequence_length}")
    
    def preprocess_tempo_data(self, tempo_df: pd.DataFrame) -> pd.DataFrame:
        """
        Comprehensive preprocessing of TEMPO satellite data
        
        Args:
            tempo_df: Raw TEMPO data DataFrame
            
        Returns:
            Cleaned and preprocessed TEMPO data
        """
        logger.info("Starting TEMPO data preprocessing...")
        
        # Make a copy to avoid modifying original data
        processed_df = tempo_df.copy()
        
        # Handle missing values
        processed_df = self.handle_missing_values(processed_df)
        
        # Remove outliers
        processed_df = self.remove_outliers(processed_df)
        
        # Create derived features
        processed_df = self.create_derived_features(processed_df)
        
        # Extract temporal features
        processed_df = self.extract_temporal_features(processed_df)
        
        # Apply quality filtering
        processed_df = self.apply_quality_filtering(processed_df)
        
        logger.info(f"TEMPO preprocessing complete. Final shape: {processed_df.shape}")
        return processed_df
    
    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Handle missing values using multiple strategies
        """
        logger.info("Handling missing values...")
        
        # Identify numeric columns (satellite measurements)
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Remove timestamp columns if present
        measurement_cols = [col for col in numeric_cols 
                          if not any(x in col.lower() for x in ['time', 'index', 'id', 'flag'])]
        
        # Strategy 1: Forward fill for short gaps (< 2 hours)
        for col in measurement_cols:
            df[col] = df[col].fillna(method='ffill', limit=2)
        
        # Strategy 2: Linear interpolation for medium gaps (2-6 hours)  
        for col in measurement_cols:
            df[col] = df[col].interpolate(method='linear', limit=6)
        
        # Strategy 3: KNN imputation for remaining gaps
        if df[measurement_cols].isnull().any().any():
            logger.info("Applying KNN imputation for remaining missing values...")
            df_imputed = pd.DataFrame(
                self.knn_imputer.fit_transform(df[measurement_cols]),
                columns=measurement_cols,
                index=df.index
            )
            df[measurement_cols] = df_imputed
        
        # Log missing data statistics
        missing_stats = df.isnull().sum()
        if missing_stats.sum() > 0:
            logger.warning(f"Remaining missing values: {missing_stats[missing_stats > 0]}")
        
        return df
    
    def remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Remove outliers using statistical methods
        """
        logger.info("Removing outliers...")
        
        measurement_cols = self._get_measurement_columns(df)
        original_count = len(df)
        
        # Method 1: Z-score based outlier removal
        for col in measurement_cols:
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            outlier_mask = z_scores > self.quality_thresholds['outlier_z_score']
            df.loc[df.index[outlier_mask], col] = np.nan
        
        # Method 2: IQR-based outlier detection for extreme values
        for col in measurement_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 2.5 * IQR
            upper_bound = Q3 + 2.5 * IQR
            
            outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
            df.loc[outlier_mask, col] = np.nan
        
        # Re-impute outlier values
        if df[measurement_cols].isnull().any().any():
            df[measurement_cols] = self.knn_imputer.fit_transform(df[measurement_cols])
        
        logger.info(f"Outlier removal complete. Processed {original_count} samples")
        return df
    
    def create_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create derived features from TEMPO measurements
        """
        logger.info("Creating derived features...")
        
        # Pollution Index (composite measure)
        if all(col in df.columns for col in ['NO2_column', 'O3_column', 'SO2_column']):
            df['pollution_index'] = (
                0.4 * self._normalize_feature(df['NO2_column']) +
                0.3 * self._normalize_feature(df['O3_column']) + 
                0.3 * self._normalize_feature(df['SO2_column'])
            )
        
        # Atmospheric clarity index
        if 'aerosol_index' in df.columns and 'cloud_fraction' in df.columns:
            df['clarity_index'] = (1 - df['cloud_fraction']) / (1 + df['aerosol_index'])
        
        # Solar correction factor
        if 'solar_zenith_angle' in df.columns:
            df['solar_correction'] = np.cos(np.radians(df['solar_zenith_angle']))
        
        # Viewing geometry factor
        if 'viewing_zenith_angle' in df.columns:
            df['viewing_correction'] = np.cos(np.radians(df['viewing_zenith_angle']))
        
        # Photochemical activity index
        if 'NO2_column' in df.columns and 'HCHO_column' in df.columns:
            df['photochemical_index'] = df['NO2_column'] * df['HCHO_column']
        
        logger.info(f"Created {5} derived features")
        return df
    
    def extract_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Extract temporal features from timestamp
        """
        if 'timestamp' not in df.columns:
            logger.warning("No timestamp column found, skipping temporal feature extraction")
            return df
            
        logger.info("Extracting temporal features...")
        
        # Ensure timestamp is datetime
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # Basic temporal features
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['month'] = df['timestamp'].dt.month
        df['season'] = df['timestamp'].dt.month % 12 // 3 + 1
        
        # Cyclical features for better ML performance
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24) 
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # Rush hour indicators
        df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9) | 
                             (df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)
        
        # Weekend indicator
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        
        logger.info("Temporal feature extraction complete")
        return df
    
    def apply_quality_filtering(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Apply data quality filtering based on quality flags
        """
        if 'quality_flag' in df.columns:
            logger.info("Applying quality filtering...")
            
            # Keep only good and marginal quality data (flags 0 and 1)
            good_quality_mask = df['quality_flag'] <= 1
            filtered_df = df[good_quality_mask].copy()
            
            removed_count = len(df) - len(filtered_df)
            logger.info(f"Removed {removed_count} poor quality samples")
            
            return filtered_df
        
        return df
    
    def fuse_data_sources(self, 
                         tempo_data: pd.DataFrame,
                         epa_data: Optional[pd.DataFrame] = None,
                         weather_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """
        Fuse multiple data sources using spatial-temporal alignment
        
        Args:
            tempo_data: Processed TEMPO satellite data
            epa_data: EPA ground station data
            weather_data: Weather data
            
        Returns:
            Fused dataset ready for ML training
        """
        logger.info("Starting multi-source data fusion...")
        
        # Start with TEMPO data as base
        fused_data = tempo_data.copy()
        
        # Fuse EPA ground station data
        if epa_data is not None:
            fused_data = self._fuse_epa_data(fused_data, epa_data)
        
        # Fuse weather data
        if weather_data is not None:
            fused_data = self._fuse_weather_data(fused_data, weather_data)
        
        # Create interaction features
        fused_data = self._create_interaction_features(fused_data)
        
        logger.info(f"Data fusion complete. Final shape: {fused_data.shape}")
        return fused_data
    
    def create_sequences_for_training(self, df: pd.DataFrame, 
                                    target_column: str = 'pollution_index') -> Tuple[np.ndarray, np.ndarray]:
        """
        Create sequences for time series training
        
        Args:
            df: Processed data DataFrame
            target_column: Name of target variable column
            
        Returns:
            Tuple of (X, y) arrays for model training
        """
        logger.info(f"Creating sequences with length {self.sequence_length}...")
        
        # Select feature columns (exclude target and metadata)
        feature_cols = [col for col in df.columns 
                       if col not in [target_column, 'timestamp', 'quality_flag']]
        
        # Prepare feature matrix
        feature_data = df[feature_cols].values
        target_data = df[target_column].values
        
        # Scale features
        scaled_features = self.scaler.fit_transform(feature_data)
        
        # Create sequences
        X, y = [], []
        
        for i in range(len(scaled_features) - self.sequence_length):
            # Input sequence
            X.append(scaled_features[i:(i + self.sequence_length)])
            # Target (next value)
            y.append(target_data[i + self.sequence_length])
        
        X = np.array(X)
        y = np.array(y)
        
        logger.info(f"Created {len(X)} training sequences with shape {X.shape}")
        return X, y
    
    def _get_measurement_columns(self, df: pd.DataFrame) -> List[str]:
        """Get measurement columns from DataFrame"""
        tempo_measurements = [
            'NO2_column', 'O3_column', 'HCHO_column', 'SO2_column',
            'aerosol_index', 'cloud_fraction', 'solar_zenith_angle',
            'viewing_zenith_angle'
        ]
        return [col for col in tempo_measurements if col in df.columns]
    
    def _normalize_feature(self, series: pd.Series) -> pd.Series:
        """Normalize feature to 0-1 range"""
        return (series - series.min()) / (series.max() - series.min())
    
    def _fuse_epa_data(self, tempo_df: pd.DataFrame, epa_df: pd.DataFrame) -> pd.DataFrame:
        """Fuse EPA ground station data"""
        logger.info("Fusing EPA ground station data...")
        # Implementation would include spatial-temporal matching
        # For now, return original data
        return tempo_df
    
    def _fuse_weather_data(self, tempo_df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:
        """Fuse weather data"""
        logger.info("Fusing weather data...")
        # Implementation would include weather parameter addition
        return tempo_df
    
    def _create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create interaction features between different data sources"""
        # Example: pollution * weather interactions
        if 'pollution_index' in df.columns and 'temperature' in df.columns:
            df['pollution_temp_interaction'] = df['pollution_index'] * df['temperature']
        
        return df
    
    def get_data_quality_report(self, df: pd.DataFrame) -> Dict:
        """
        Generate comprehensive data quality report
        
        Args:
            df: Processed DataFrame
            
        Returns:
            Dictionary containing quality metrics
        """
        report = {
            'total_samples': len(df),
            'feature_count': len(df.columns),
            'missing_data_percentage': df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100,
            'temporal_coverage': {
                'start_time': df['timestamp'].min() if 'timestamp' in df.columns else 'Unknown',
                'end_time': df['timestamp'].max() if 'timestamp' in df.columns else 'Unknown',
                'total_hours': len(df) if 'timestamp' in df.columns else 'Unknown'
            },
            'feature_statistics': {}
        }
        
        # Calculate statistics for measurement columns
        measurement_cols = self._get_measurement_columns(df)
        for col in measurement_cols:
            if col in df.columns:
                report['feature_statistics'][col] = {
                    'mean': float(df[col].mean()),
                    'std': float(df[col].std()),
                    'missing_percentage': float(df[col].isnull().sum() / len(df) * 100)
                }
        
        return report


def main():
    """Demonstrate the data processing pipeline"""
    print("ðŸ”§ Air Quality Data Processing Pipeline - ML Engineer Task 2")
    print("=" * 70)
    
    # Initialize processor
    processor = AirQualityDataProcessor(sequence_length=24)
    
    # Create mock TEMPO data for demonstration
    np.random.seed(42)
    n_samples = 100
    
    mock_tempo_data = pd.DataFrame({
        'timestamp': pd.date_range('2024-09-21 06:00:00', periods=n_samples, freq='H'),
        'NO2_column': np.random.lognormal(0.5, 0.8, n_samples),
        'O3_column': np.random.lognormal(1.2, 0.6, n_samples),
        'HCHO_column': np.random.lognormal(-0.3, 0.7, n_samples),
        'SO2_column': np.random.lognormal(-1.0, 0.9, n_samples),
        'aerosol_index': np.random.normal(0.5, 0.3, n_samples),
        'cloud_fraction': np.random.beta(2, 3, n_samples),
        'solar_zenith_angle': np.random.uniform(20, 80, n_samples),
        'viewing_zenith_angle': np.random.uniform(0, 60, n_samples),
        'quality_flag': np.random.choice([0, 1, 2], n_samples, p=[0.85, 0.1, 0.05])
    })
    
    # Add some missing values for testing
    missing_indices = np.random.choice(n_samples, size=5, replace=False)
    mock_tempo_data.loc[missing_indices, 'NO2_column'] = np.nan
    
    print(f"ðŸ“Š Input data shape: {mock_tempo_data.shape}")
    print(f"ðŸ“Š Missing values: {mock_tempo_data.isnull().sum().sum()}")
    
    # Process the data
    print("\nðŸ”„ Processing TEMPO data...")
    processed_data = processor.preprocess_tempo_data(mock_tempo_data)
    
    print(f"âœ… Processed data shape: {processed_data.shape}")
    print(f"âœ… Missing values after processing: {processed_data.isnull().sum().sum()}")
    
    # Create training sequences
    print("\nðŸ“ˆ Creating training sequences...")
    X, y = processor.create_sequences_for_training(processed_data)
    
    print(f"âœ… Training sequences created:")
    print(f"   X shape: {X.shape}")
    print(f"   y shape: {y.shape}")
    
    # Generate quality report
    print("\nðŸ“‹ Generating data quality report...")
    quality_report = processor.get_data_quality_report(processed_data)
    
    print("Data Quality Report:")
    print(f"  Total samples: {quality_report['total_samples']}")
    print(f"  Features: {quality_report['feature_count']}")
    print(f"  Missing data: {quality_report['missing_data_percentage']:.2f}%")
    print(f"  Temporal coverage: {quality_report['temporal_coverage']['total_hours']} hours")
    
    print("\nâœ… Data processing pipeline demonstration complete!")
    print("Ready for LSTM model training.")
    
    return processed_data, X, y

if __name__ == "__main__":
    processed_data, X, y = main()